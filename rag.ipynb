{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94e27c11-d02f-43bc-b6cc-cdcc961382c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f351b6d-1901-4f01-aaa1-1de11a115136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from langchain import hub\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    PromptTemplate,\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    SummaryIndex,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.base.base_query_engine import BaseQueryEngine\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.prompts import LangchainPromptTemplate\n",
    "from llama_index.core.prompts.base import BasePromptTemplate\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fae3f559-6b4a-4cac-a098-c7e338c66b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "REGION = \"us-central1\" \n",
    "GCS_BUCKET = \"vertex_ai_rag_bucket\"\n",
    "VS_INDEX_NAME = \"llamaindex_doc_index\"\n",
    "VS_INDEX_ENDPOINT_NAME = \"llamaindex_doc_endpoint\"\n",
    "DOC_FOLDER = \"./data_tutorial\"\n",
    "DOC_FOLDER_NEW = \"./data_original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b7bc3e-435e-4eb0-a4d5-51492d38256b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://github-repo/generative-ai/gemini/use-cases/retrieval-augmented-generation/llamaindex/data/04a01.pdf to file://./data_tutorial/04a01.pdf\n",
      "Copying gs://github-repo/generative-ai/gemini/use-cases/retrieval-augmented-generation/llamaindex/data/04a02.pdf to file://./data_tutorial/04a02.pdf\n",
      "Copying gs://github-repo/generative-ai/gemini/use-cases/retrieval-augmented-generation/llamaindex/data/04a03.pdf to file://./data_tutorial/04a03.pdf\n",
      "Copying gs://github-repo/generative-ai/gemini/use-cases/retrieval-augmented-generation/llamaindex/data/04a04.pdf to file://./data_tutorial/04a04.pdf\n",
      "  Completed files 4/4 | 3.2MiB/3.2MiB                                          \n",
      "\n",
      "Average throughput: 35.7MiB/s\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!mkdir {DOC_FOLDER}\n",
    "!gcloud storage cp gs://github-repo/generative-ai/gemini/use-cases/retrieval-augmented-generation/llamaindex/data/* {DOC_FOLDER}\n",
    "\n",
    "print(\"Download completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1be2bd-c71c-4c78-8286-0d770a52c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"./scripts\")\n",
    "\n",
    "from scripts import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3bb5553-279b-451b-b063-e6beda15d5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    # The number of dimensions for the gecko text embeddings is 768\n",
    "    VS_DIMENSIONS = 768\n",
    "    \n",
    "    # Vertex AI Vector Search Index configuration\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    new_bucket = bucket = storage_client.bucket(GCS_BUCKET)\n",
    "    \n",
    "    # https://cloud.google.com/vertex-ai/docs/vector-search/overview\n",
    "    vs_index = utils.create_vector_search_index(VS_INDEX_NAME, VS_DIMENSIONS)\n",
    "    vs_endpoint = utils.create_vector_search_endpoint(VS_INDEX_ENDPOINT_NAME)\n",
    "    vs_deployed_index = utils.deploy_vector_search_endpoint(\n",
    "        vs_index, vs_endpoint, VS_INDEX_NAME\n",
    "    )\n",
    "\n",
    "    return new_bucket, vs_index, vs_endpoint, vs_deployed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b99a6c8e-3672-43c9-aee4-f28f4e510f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a service account for using text embedding and llm models in gcp - add to terraform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c63de256-5543-427f-9d3e-510e2897cf07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_llm_and_storage(\n",
    "    vs_index: aiplatform.MatchingEngineIndex,\n",
    "    vs_endpoint: aiplatform.MatchingEngineIndexEndpoint,\n",
    ") -> StorageContext:\n",
    "    \"\"\"\n",
    "    Initializes Vertex AI Vector Store given a Vector Search index and deployed endpoint.\n",
    "    Configures embedding and LLMs models to be gecko and Gemini.\n",
    "    \"\"\"\n",
    "    # setup storage\n",
    "    vector_store = VertexAIVectorStore(\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        index_id=vs_index.resource_name,\n",
    "        endpoint_id=vs_endpoint.resource_name,\n",
    "        gcs_bucket_name=GCS_BUCKET,\n",
    "    )\n",
    "\n",
    "    # set storage context\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    from google.oauth2 import service_account\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        \"creds/my-practice-setting-c57b3dab3a57.json\"\n",
    "    )\n",
    "    \n",
    "    gemini_embedding_model = VertexTextEmbedding(\n",
    "        model_name=\"text-embedding-005\",\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        credentials=credentials,\n",
    "    )\n",
    "    llm = Vertex(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        project=PROJECT_ID, \n",
    "        credentials=credentials\n",
    "    )\n",
    "\n",
    "    Settings.embed_model = gemini_embedding_model\n",
    "    Settings.llm = llm\n",
    "\n",
    "    return storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10cc58a6-6589-4fa6-a2cd-f60e6553a521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vector Search index llamaindex_doc_index ...\n",
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/535465771333/locations/us-central1/indexes/125698368310607872/operations/5358837945123995648\n",
      "MatchingEngineIndex created. Resource name: projects/535465771333/locations/us-central1/indexes/125698368310607872\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/535465771333/locations/us-central1/indexes/125698368310607872')\n",
      "Vector Search index llamaindex_doc_index created with resource name projects/535465771333/locations/us-central1/indexes/125698368310607872\n",
      "Creating Vector Search index endpoint llamaindex_doc_endpoint ...\n",
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816/operations/8884874978378383360\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816')\n",
      "Vector Search index endpoint llamaindex_doc_endpoint created with resource name projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816\n",
      "Deploying Vector Search index llamaindex_doc_index at endpoint llamaindex_doc_endpoint ...\n",
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816/operations/6742850405610291200\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/535465771333/locations/us-central1/indexEndpoints/5336662204341026816\n",
      "Vector Search index llamaindex_doc_index is deployed at endpoint llamaindex_doc_endpoint\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Either provide credentials or all of client_email, token_uri, private_key_id, and private_key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m (bucket, vs_index, vs_endpoint, deployed_endpoint) \u001b[38;5;241m=\u001b[39m setup()\n\u001b[0;32m----> 2\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_llm_and_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvs_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvs_endpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m docs \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(DOC_FOLDER)\u001b[38;5;241m.\u001b[39mload_data()\n",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m, in \u001b[0;36minitialize_llm_and_storage\u001b[0;34m(vs_index, vs_endpoint)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# set storage context\u001b[39;00m\n\u001b[1;32m     19\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m---> 21\u001b[0m gemini_embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mVertexTextEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-005\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m llm \u001b[38;5;241m=\u001b[39m Vertex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m Settings\u001b[38;5;241m.\u001b[39membed_model \u001b[38;5;241m=\u001b[39m gemini_embedding_model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/embeddings/vertex/base.py:152\u001b[0m, in \u001b[0;36mVertexTextEmbedding.__init__\u001b[0;34m(self, model_name, project, location, credentials, embed_mode, embed_batch_size, callback_manager, additional_kwargs, num_workers, client_email, token_uri, private_key_id, private_key)\u001b[0m\n\u001b[1;32m    148\u001b[0m         credentials \u001b[38;5;241m=\u001b[39m service_account\u001b[38;5;241m.\u001b[39mCredentials\u001b[38;5;241m.\u001b[39mfrom_service_account_info(\n\u001b[1;32m    149\u001b[0m             info\n\u001b[1;32m    150\u001b[0m         )\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither provide credentials or all of client_email, token_uri, private_key_id, and private_key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    156\u001b[0m init_vertexai(project\u001b[38;5;241m=\u001b[39mproject, location\u001b[38;5;241m=\u001b[39mlocation, credentials\u001b[38;5;241m=\u001b[39mcredentials)\n\u001b[1;32m    157\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManager([])\n",
      "\u001b[0;31mValueError\u001b[0m: Either provide credentials or all of client_email, token_uri, private_key_id, and private_key."
     ]
    }
   ],
   "source": [
    "(bucket, vs_index, vs_endpoint, deployed_endpoint) = setup()\n",
    "storage_context = initialize_llm_and_storage(vs_index, vs_endpoint)\n",
    "\n",
    "# Simply pass in a input directory or a list of files. It will select the best file reader based on the \n",
    "# file extensions. \n",
    "docs = SimpleDirectoryReader(DOC_FOLDER).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db227e7-c0f5-4b73-80df-1e4d465e3bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
